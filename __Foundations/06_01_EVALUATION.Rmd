# Model Evaluation


## Classification Models: Evaluation

[My medium story](https://medium.com/@demrahayan/evaluating-binary-classification-models-with-pyspark-2afc5ac7937f)

[Google developers](https://developers.google.com/machine-learning/crash-course/classification/)

### Thresholding

Logistic regression returns a probability. You can use the returned probability "as is" (for example, the probability that the user will click on this ad is 0.00023) or convert the returned probability to a binary value (for example, this email is spam).

A logistic regression model that returns 0.9995 for a particular email message is predicting that it is very likely to be spam. Conversely, another email message with a prediction score of 0.0003 on that same logistic regression model is very likely not spam.

However, what about an email message with a prediction score of 0.6? In order to map a logistic regression value to a binary category, you must define a `classification threshold` (also called the `decision threshold`).

A value above that threshold indicates "spam"; a value below indicates "not spam." It is tempting to assume that the classification threshold should always be 0.5, but thresholds are problem-dependent, and are therefore values that you must tune.

Note: "Tuning" a threshold for logistic regression is different from tuning hyperparameters such as learning rate. Part of choosing a threshold is assessing how much you'll suffer for making a mistake. For example, mistakenly labeling a non-spam message as spam is very bad. However, mistakenly labeling a spam message as non-spam is unpleasant, but hardly the end of your job.

### Confusion Matrix

|                     | Predicted Positive | Predicted Negative |
|---------------------|--------------------|--------------------|
| **Actual Positive** | TP                 | FN                 |
| **Actual Negative** | FP                 | TN                 |

**True Positive:** Model predicted positive and it is true.

**True negative:** Model predicted negative and it is true.

**False positive (Type 1 Error):** Model predicted positive but it is false.

**False negative (Type 2 Error):** Model predicted negative and it is true.



#### False Positive Rate (FPR):

The False Positive Rate is the ratio of false positive predictions to the total number of actual negatives. It measures the rate at which the model incorrectly predicts the positive class among the instances that are actually negative.

$FPR = \frac{FP}{TN + FP}$


#### True Positive Rate (TPR), Sensitivity, or Recall:

The True Positive Rate is the ratio of true positive predictions to the total number of actual positives. It measures the ability of the model to correctly predict the positive class among instances that are actually positive.

Recall (TPR) $= \frac{TP}{TP + FN}$


#### Accuracy:

It represents the ratio of correctly predicted instances to the total number of instances. The accuracy metric is suitable for balanced datasets where the classes are evenly distributed. It is calculated using the following formula:

Accuracy $= \frac{TP + TN}{TP + TN + FP + FN}$

Accuracy provides a general sense of how well a model is performing across all classes. It is easy to understand and interpret, making it a commonly used metric, especially when the classes are balanced.

However, accuracy may not be an ideal metric in situations where the class distribution is imbalanced. In imbalanced datasets, where one class significantly outnumbers the other, a high accuracy might be achieved by simply predicting the majority class. In such cases, other metrics like precision, recall, F1 score, or area under the receiver operating characteristic (ROC-AUC) curve may be more informative.


#### Precision: 

Precision is the ratio of true positive predictions to the total number of positive predictions made by the model. It represents the accuracy of the positive predictions made by the model. 

$Precision = \frac{TP}{TP + FP}$


**F1 Measure:**

The F1 score is a metric commonly used in binary classification to provide a balance between precision and recall. It is the harmonic mean of precision and recall, combining both measures into a single value. The F1 score is particularly useful when there is an uneven class distribution or when both false positives and false negatives are important considerations.

The F1 score is useful in situations where achieving a balance between precision and recall is important, as it penalizes models that have a significant imbalance between these two metrics.

$F1 score = 2 \times \frac{Precision \times Recall}{Precision + Recall}$



#### In Marketing

In marketing, the choice between optimizing for `precision` or `recall` depends on the specific business objectives and the costs associated with false positives and false negatives:

Precision is prioritized when the cost of targeting non-lookalikes is high, and we want to ensure that most of the targeted individuals are genuine lookalikes.

Recall is prioritized when the cost of missing a potential lookalike (lost opportunity) is high, and we want to capture as many true lookalikes as possible, even if it means including some non-lookalikes.

The decision on which metric to prioritize is driven by the campaign's context and goals."

**Precision:**

Precision is the ratio of correctly identified positives (true lookalikes) to all instances that were predicted as positives (both true and false lookalikes).

In marketing, precision is valuable when the cost or impact associated with false positives (incorrectly identifying a non-lookalike as a lookalike) is high.

Example: If targeting a non-lookalike with a marketing campaign incurs significant costs (e.g., sending out costly promotions or ads to uninterested users), you want to minimize false positives. High precision ensures that most of the people you target are actual lookalikes, thus reducing wasted marketing spend.


**Recall:**

Recall is the ratio of correctly identified positives (true lookalikes) to all actual positives (true lookalikes).

In marketing, recall is important when you want to ensure that you are not missing potential opportunities (actual lookalikes).

Example: If missing a true lookalike (a customer who is likely to respond positively to a campaign) results in a high cost or lost opportunity (e.g., missed revenue or engagement), you want to maximize recall. High recall ensures that most of the potential lookalikes are captured by the model, even if some non-lookalikes are incorrectly included.


## ROC Curve


The **ROC (Receiver Operating Characteristic) curve** is a graphical tool used to evaluate the performance of binary classification models. It helps in understanding how well a model distinguishes between two classes.

The ROC curve helps visualize the trade-offs between true positive rate and false positive rate across different thresholds. By analyzing the ROC curve, considering business costs, and using metrics like Youden’s Index, you can select a probability threshold that balances performance according to your specific needs.


### Components of the ROC Curve:

- **True Positive Rate (TPR) / Sensitivity / Recall**:
 
- Measures the proportion of actual positive cases that are correctly identified by the model.

- Formula:
       \[
       \text{TPR} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
       \]

- **False Positive Rate (FPR)**:

- Measures the proportion of actual negative cases that are incorrectly classified as positive by the model.

- Formula:
       \[
       \text{FPR} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}
       \]

### How to Read the ROC Curve:

- **X-axis**: False Positive Rate (FPR) – The rate at which negative cases are incorrectly classified as positive.

- **Y-axis**: True Positive Rate (TPR) – The rate at which positive cases are correctly identified.

- A **perfect classifier** would be represented by a point in the upper-left corner of the plot, indicating high TPR and low FPR.

- A **random classifier** would produce a diagonal line from the bottom-left to the top-right of the plot, indicating no discriminative power.

### Area Under the ROC Curve (AUC):

- **AUC** represents the probability that the model ranks a randomly chosen positive case higher than a randomly chosen negative case.

- **AUC Values**:

  - **AUC = 0.5**: No discriminative power (model performs as well as random guessing).
  
  - **0.7 < AUC < 0.8**: Fair performance.
  
  - **0.8 < AUC < 0.9**: Good performance.
  
  - **AUC > 0.9**: Excellent performance.

### Applications of ROC Curve:

- **Model Evaluation**: The ROC curve helps compare different models and choose the one with the best trade-off between true positive rate and false positive rate.

- **Threshold Selection**: It aids in selecting the optimal probability threshold for classification, balancing the rate of true positives and false positives.


### Using the ROC Curve in Real Examples

The ROC curve is a valuable tool for evaluating and selecting the optimal probability threshold in binary classification problems. Here’s how you can use the ROC curve in practice and select an appropriate threshold:


#### Train Your Model:

   - Fit your binary classification model to the training data.

#### Predict Probabilities:

   - Use the model to predict probabilities of the positive class for the test data. These probabilities are used to assess the performance of the model at various thresholds.

#### Compute True Positive Rate (TPR) and False Positive Rate (FPR):

   - For different threshold values (from 0 to 1), calculate the TPR and FPR. This involves varying the classification threshold and computing the confusion matrix for each threshold.

#### Plot the ROC Curve:

   - Plot the TPR (on the y-axis) against the FPR (on the x-axis) for each threshold value. This gives you the ROC curve.

#### Calculate the Area Under the Curve (AUC):
   
   - The AUC provides a summary measure of the model’s performance. A higher AUC indicates better model performance.

### Selecting the Probability Threshold:

Choosing the right probability threshold is crucial for optimizing your model's performance based on your specific needs. Here’s how to select an appropriate threshold:

#### Visual Inspection:

   - **Look at the ROC Curve:** Find the point on the ROC curve that is closest to the top-left corner (where TPR is high and FPR is low). This point represents a good trade-off between sensitivity and specificity.

#### Consider the Business Context:

   - **Cost-Benefit Analysis:** If the cost of false positives is high (e.g., wasted marketing spend), you might prefer a threshold that minimizes FPR. Conversely, if missing true positives is costly (e.g., lost revenue), you might choose a lower threshold to increase TPR.
   
   - **Decision-Making Criteria:** Determine the acceptable levels of TPR and FPR based on business requirements. For example, in a medical diagnosis context, you might prefer higher recall (sensitivity) to ensure no patient with a condition is missed, even if it means higher false positives.

#### Optimization Metrics:

- **Youden’s Index:** Calculate Youden’s Index (\( J \)) which is defined as:
     \[
     J = \text{TPR} - \text{FPR}
     \]
     The threshold corresponding to the maximum value of \( J \) can be chosen as it represents the best trade-off between TPR and FPR.

#### Confusion Matrix Analysis:

   - **Evaluate Different Thresholds:** For each threshold, compute the confusion matrix and analyze precision, recall, and F1-score. Choose the threshold that best aligns with your performance goals.

#### Cross-Validation:

   - **Cross-Validation:** Use cross-validation to ensure that the chosen threshold performs well across different subsets of the data. This helps in generalizing the model's performance and avoiding overfitting.

### ROC Curve Example:

Let’s consider an example where you have a model predicting whether an email is spam or not:

1. **Train the Model:** You train a logistic regression model to classify emails as spam or not spam.

2. **Predict Probabilities:** The model outputs probabilities for each email being spam.

3. **Compute TPR and FPR:** Calculate TPR and FPR for various thresholds (e.g., 0.1, 0.2, ..., 0.9).

4. **Plot the ROC Curve:** Plot TPR against FPR for each threshold value.

5. **Select Threshold:**

   - **Visual Inspection:** Identify the threshold where the ROC curve is closest to the top-left corner.

   - **Business Context:** If false positives (non-spam emails marked as spam) lead to user dissatisfaction, you might prefer a higher threshold to reduce FPR.

   - **Optimization:** Calculate Youden’s Index and select the threshold with the highest value.

6. **Implement and Monitor:** Set the chosen threshold in your production system and monitor its performance. Adjust as needed based on real-world feedback and performance metrics.

