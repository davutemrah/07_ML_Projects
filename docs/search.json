[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Hello ! üëã devoted explorer navigating expansive realm machine learning, delighted present personal repository‚Äîvirtual haven houses notes, musings, sample projects sourced diverse array blogs, books, practical encounters.curated collection serves mosaic insights, codes notes thoughtfully extracted publicly available machine learning blogs. project within repository testament ongoing quest understanding, meticulously pieced together rich tapestry digital knowledge landscape.Whether fellow enthusiast, curious mind, seasoned practitioner, extend invitation explore codebase, delve concepts, perhaps find inspiration machine learning journey. repository merely repository algorithms snippets; reflection commitment, curiosity, enthusiasm ever-evolving field machine learning.encourage engage, share thoughts, even collaborate journey. Let‚Äôs celebrate collaborative spirit machine learning community together, embrace boundless opportunities arise fusion code, data, collective wisdom publicly available resources.Happy exploration!","code":""},{"path":"projects.html","id":"projects","chapter":"1 Projects","heading":"1 Projects","text":"DataTab Statistics Tutorials","code":""},{"path":"machine-learning-fundamentals.html","id":"machine-learning-fundamentals","chapter":"2 Machine Learning Fundamentals","heading":"2 Machine Learning Fundamentals","text":"","code":""},{"path":"machine-learning-fundamentals.html","id":"definitions","chapter":"2 Machine Learning Fundamentals","heading":"2.1 definitions","text":"","code":""},{"path":"machine-learning-fundamentals.html","id":"data-science","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1 Data Science","text":"","code":""},{"path":"machine-learning-fundamentals.html","id":"what-is-data-science","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.1 What is data science?","text":"core, data science using data answer questions. pretty broad definition, ‚Äôs ‚Äôs pretty broad field!Data science can involve:‚Ä¢ Statistics, computer science, mathematics\n‚Ä¢ Data cleaning formatting\n‚Ä¢ Data visualization\nEconomist Special Report sums m√©lange skills well - state data scientist broadly defined someone:\n‚Äúcombines skills software programmer, statistician storyteller slash artist extract nuggets gold hidden mountains data‚Äù\nend courses, hopefully feel equipped just !","code":""},{"path":"machine-learning-fundamentals.html","id":"why-do-we-need-data-science","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.2 Why do we need data science?","text":"One reasons rise data science recent years vast amount data currently available generated. massive amounts data collected many aspects world lives, simultaneously rise inexpensive computing. created perfect storm rich data tools analyse : Rising computer memory capabilities, better processors, software now, data scientists skills put use answer questions using data!\nlittle anecdote describes truly exponential growth data generation experiencing. third century BC, Library Alexandria believed house sum human knowledge. Today, enough information world give every person alive 320 times much historians think stored Alexandria‚Äôs entire collection.\nstill growing.","code":""},{"path":"machine-learning-fundamentals.html","id":"what-is-big-data","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.3 What is big data?","text":"integral rise data science. qualities characterize big data. first volume. name implies, big data involves large datasets - large datasets becoming routine. example, say question online video - well, YouTube approximately 300 hours video uploaded every minute! definitely lot data available analyse, can see might difficult problem wrangle data!brings us second quality big data: velocity. Data generated collected faster ever . YouTube example, new data coming every minute! completely different example, say question shipping times routes. Well, transport trucks real time GPS data available - real time analyse trucks movements‚Ä¶ tools skills !third quality big data variety. examples ‚Äôve mentioned far, different types data available . YouTube example, analysing video audio, unstructured data set, database video lengths, views comments, much structured dataset analyse.","code":""},{"path":"machine-learning-fundamentals.html","id":"descriptive-analysis","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.4 1. Descriptive analysis","text":"goal descriptive analysis describe summarize set data. Whenever get new dataset examine, usually first kind analysis perform. Descriptive analysis generate simple summaries samples measurements. may familiar common descriptive statistics: measures central tendency (eg: mean, median, mode) measures variability (eg: range, standard deviations variance).\ntype analysis aimed summarizing sample ‚Äì generalizing results analysis larger population trying make conclusions. Description data separated making interpretations; generalizations interpretations require additional statistical steps.\nexamples purely descriptive analysis can seen censuses. , government collects series measurements country‚Äôs citizens, can summarized. , shown age distribution US, stratified sex.","code":""},{"path":"machine-learning-fundamentals.html","id":"exploratory-analysis","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.5 2. Exploratory analysis","text":"goal exploratory analysis examine explore data find relationships weren‚Äôt previously known. Exploratory analyses explore different measures might related confirm relationship causitive. ‚Äôve probably heard phrase ‚ÄúCorrelation imply causation‚Äù exploratory analyses lie root saying. Just observe relationship two variables exploratory analysis, mean one necessarily causes .\n, exploratory analyses, useful discovering new connections, final say answering question! can allow formulate hypotheses drive design future studies data collection, exploratory analysis alone never used final say data might related .","code":""},{"path":"machine-learning-fundamentals.html","id":"inferential-analysis","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.6 3. Inferential analysis","text":"goal inferential analyses use relatively small sample data infer say something population large. Inferential analysis commonly goal statistical modelling, small amount information extrapolate generalize information larger group.Inferential analysis typically involves using data estimate value population give measure uncertainty estimate. Since moving small amount data trying generalize larger population, ability accurately infer information larger population depends heavily sampling scheme - data collect representative sample population, generalizations infer won‚Äôt accurate population.","code":""},{"path":"machine-learning-fundamentals.html","id":"predictive-analysis","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.7 4. Predictive analysis","text":"goal predictive analysis use current data make predictions future data. Essentially, using current historical data find patterns predict likelihood future outcomes.\nLike inferential analysis, accuracy predictions dependent measuring right variables. aren‚Äôt measuring right variables predict outcome, predictions aren‚Äôt going accurate. Additionally, many ways build prediction models better worse specific cases, general, data simple model generally performs well predicting future outcomes.\nsaid, much like exploratory analysis, just one variable may predict another, mean one causes ; just capitalizing observed relationship predict second variable.\ncommon saying prediction hard, especially future. aren‚Äôt easy ways gauge well going predict event event come pass; evaluating different approaches models challenge.spend lot time trying predict things - upcoming weather, outcomes sports events, example ‚Äôll explore , outcomes elections. ‚Äôve previously mentioned Nate Silver FiveThirtyEight, try predict outcomes U.S. elections (sports matches, !). Using historical polling data trends current polling, FiveThirtyEight builds models predict outcomes next US Presidential vote - fairly accurate ! FiveThirtyEight‚Äôs models accurately predicted 2008 2012 elections widely considered outlier 2016 US elections, one models suggest Donald Trump chance winning.","code":""},{"path":"machine-learning-fundamentals.html","id":"causal-analysis","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.8 Causal analysis","text":"caveat lot analyses ‚Äôve looked far can see correlations can‚Äôt get cause relationships observe. Causal analysis fills gap; goal causal analysis see happens one variable manipulate another variable - looking cause effect relationship.\nGenerally, causal analyses fairly complicated observed data alone; always questions whether correlation driving conclusions assumptions underlying analysis valid. often, causal analyses applied results randomized studies designed identify causation. Causal analysis often considered gold standard data analysis, seen frequently scientific studies scientists trying identify cause phenomenon, often getting appropriate data causal analysis challenge.\nOne thing note causal analysis data usually analysed aggregate observed relationships usually average effects; , average giving certain population drug may alleviate symptoms disease, causal relationship may hold true every single affected individual.","code":""},{"path":"machine-learning-fundamentals.html","id":"experimental-design","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.9 Experimental Design","text":"Now ‚Äôve looked different types data science questions, going spend time looking experimental design concepts. data scientist, scientist , need ability design proper experiments best answer data science questions!\nexperimental design mean?\nExperimental design organizing experiment correct data (enough !) clearly effectively answer data science question. process involves clearly formulating question advance data collection, designing best set-possible gather data answer question, identifying problems sources error design, , collecting appropriate data.\ncare?","code":""},{"path":"machine-learning-fundamentals.html","id":"confounder","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.10 Confounder:","text":"extraneous variable may affect relationship dependent independent variables.\nexample, since age affects foot size literacy affected age, see relationship shoe size literacy, relationship may actually due age ‚Äì age ‚Äúconfounding‚Äù experimental design!\ncontrol , can make sure also measure age individual can take account effects age literacy, well. Another way control age‚Äôs effect literacy fix age participants. everyone study age, removed possible effect age literacy.Age confounding experimental design! need control \nexperimental design paradigms, control group may appropriate. group experimental subjects manipulated. studying effect drug survival, group received drug (treatment) group (control). way, can compare effects drug treatment versus control group.control group group subjects receive treatment, still dependent variables measured\nstudy designs, strategies can use control confounding effects. One, can blind subjects assigned treatment group. Sometimes, subject knows treatment group (eg: receiving experimental drug), can feel better, drug , knowing receiving treatment. known placebo effect. combat , often participants blinded treatment group ; usually achieved giving control group mock treatment (eg: given sugar pill told drug). way, placebo effect causing problem experiment, groups experience equally.Blinding study means subjects don‚Äôt know group belong - participants receive ‚Äútreatment‚Äù\nstrategy heart many studies; spreading possible confounding effects equally across groups compared. example, think age possible confounding effect, making sure groups similar ages age ranges help mitigate effect age may dependent variable - effect age equal two groups.\n‚Äúbalancing‚Äù confounders often achieved randomization. Generally, don‚Äôt know confounder beforehand; help lessen risk accidentally biasing one group enriched confounder, can randomly assign individuals groups. means potential confounding variables distributed group roughly equally, help eliminate/reduce systematic errors.Randomizing subjects either control treatment group great strategy reduce confounders‚Äô effects\none final concept experimental design need cover lesson, replication. Replication pretty much sounds like, repeating experiment different experimental subjects. single experiment‚Äôs results may occured chance; confounder unevenly distributed across groups, systematic error data collection, outliers, etc. However, can repeat experiment collect whole new set data still come conclusion, study much stronger. Also heart replication allows measure variability data accurately, allows better assess whether differences see data significant.","code":""},{"path":"machine-learning-fundamentals.html","id":"beware-p-hacking","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.11 Beware p-hacking!","text":"One many things often reported experiments value called p-value. value tells probability results experiment observed chance. important concept statistics won‚Äôt covering depth , want know , check video explaining p-values.\nneed look manipulate p-values towards end. Often, p-value less 0.05 (words, 5 percent chance differences saw observed chance), result considered significant. 20 tests, chance, expect one twenty (5%) significant. age big data, testing twenty hypotheses easy proposition. term p-hacking comes : exhaustively search data set find patterns correlations appear statistically significant virtue sheer number tests performed. spurious correlations can reported significant perform enough tests, can find data set analysis show wanted see.\nCheck FiveThirtyEight activity can manipulate filter data perform series tests can get data find whatever relationship want!\nXKCD mocks concept comic testing link jelly beans acne - clearly link , test enough jelly bean colours, eventually, one correlated acne p-value < 0.05!","code":""},{"path":"machine-learning-fundamentals.html","id":"data-types","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.12 Data types","text":"Continuous variables anything measured quantitative scale fractional number. example something like weight measured kg.Continuous variables anything measured quantitative scale fractional number. example something like weight measured kg.Ordinal data data fixed, small (< 100) number levels ordered. example survey responses choices : poor, fair, good.Ordinal data data fixed, small (< 100) number levels ordered. example survey responses choices : poor, fair, good.Categorical data data multiple categories, aren‚Äôt ordered. One example sex: male female. coding attractive self-documenting.Categorical data data multiple categories, aren‚Äôt ordered. One example sex: male female. coding attractive self-documenting.Missing data data unobserved don‚Äôt know mechanism. code missing values NA.Missing data data unobserved don‚Äôt know mechanism. code missing values NA.Censored data data know missingness mechanism level. Common examples measurement detection limit patient lost follow-. also coded NA don‚Äôt data. also add new column tidy data called, ‚ÄúVariableNameCensored‚Äù values TRUE censored FALSE .Censored data data know missingness mechanism level. Common examples measurement detection limit patient lost follow-. also coded NA don‚Äôt data. also add new column tidy data called, ‚ÄúVariableNameCensored‚Äù values TRUE censored FALSE .","code":""},{"path":"machine-learning-fundamentals.html","id":"data-scientists-in-marketing-science","chapter":"2 Machine Learning Fundamentals","heading":"2.1.1.13 Data scientists in marketing science","text":"Data scientists marketing science play crucial role leveraging data-driven insights optimize marketing strategies improve decision-making.Data scientists marketing science contribute significantly development targeted, efficient, impactful marketing campaigns harnessing power data analytics. work helps organizations optimize marketing spend, enhance customer experiences, achieve measurable business outcomes.key responsibilities activities data scientists marketing science typically engage :Data Analysis:\nConducting extensive data analysis understand customer behavior, market trends, relevant metrics.\nUtilizing statistical methods machine learning algorithms extract meaningful patterns insights large datasets.\nConducting extensive data analysis understand customer behavior, market trends, relevant metrics.Utilizing statistical methods machine learning algorithms extract meaningful patterns insights large datasets.Predictive Modeling:\nDeveloping deploying predictive models forecast future trends, customer behavior, campaign outcomes.\nUsing machine learning techniques, regression analysis, decision trees, ensemble methods, build predictive models.\nDeveloping deploying predictive models forecast future trends, customer behavior, campaign outcomes.Using machine learning techniques, regression analysis, decision trees, ensemble methods, build predictive models.Segmentation Targeting:\nCreating customer segments based demographics, behavior, relevant factors.\nOptimizing marketing strategies targeting specific segments personalized relevant content.\nCreating customer segments based demographics, behavior, relevant factors.Optimizing marketing strategies targeting specific segments personalized relevant content./B Testing:\nDesigning conducting /B tests evaluate effectiveness different marketing strategies, campaigns, variations.\nAnalyzing /B test results make data-driven recommendations optimization.\nDesigning conducting /B tests evaluate effectiveness different marketing strategies, campaigns, variations.Analyzing /B test results make data-driven recommendations optimization.Causal Inference:\nApplying advanced causal inference methods understand impact marketing initiatives customer behavior.\nAssessing causal relationships marketing activities business outcomes.\nApplying advanced causal inference methods understand impact marketing initiatives customer behavior.Assessing causal relationships marketing activities business outcomes.Data Visualization:\nCreating clear compelling data visualizations communicate complex insights non-technical stakeholders.\nUsing tools like Tableau, Power BI, custom scripts visualize data meaningful way.\nCreating clear compelling data visualizations communicate complex insights non-technical stakeholders.Using tools like Tableau, Power BI, custom scripts visualize data meaningful way.Optimization Strategies:\nCollaborating marketing teams develop optimize marketing strategies based data insights.\nRecommending adjustments campaigns, targeting strategies, budget allocations better performance.\nCollaborating marketing teams develop optimize marketing strategies based data insights.Recommending adjustments campaigns, targeting strategies, budget allocations better performance.Performance Measurement:\nDeveloping key performance indicators (KPIs) metrics assess success marketing campaigns.\nMonitoring evaluating marketing performance established benchmarks.\nDeveloping key performance indicators (KPIs) metrics assess success marketing campaigns.Monitoring evaluating marketing performance established benchmarks.Data Management:\nEnsuring quality integrity marketing data cleaning, preprocessing, validating datasets.\nCollaborating data engineers design implement data pipelines efficient data processing.\nEnsuring quality integrity marketing data cleaning, preprocessing, validating datasets.Collaborating data engineers design implement data pipelines efficient data processing.Communication Collaboration:\nEffectively communicating findings insights non-technical stakeholders, including marketing teams executives.\nCollaborating cross-functional teams align data-driven strategies overall business objectives.\nEffectively communicating findings insights non-technical stakeholders, including marketing teams executives.Collaborating cross-functional teams align data-driven strategies overall business objectives.Overfitting:Statistical model complexStatistical model complexToo many parameters compared total number observations.many parameters compared total number observations.Poor Predictive PerformancePoor Predictive PerformanceOverfitted model overreacts minor fluctuations training dataOverfitted model overreacts minor fluctuations training dataUnderfitting:Statistical model primitiveStatistical model primitivePoor Predictive Performance training modelPoor Predictive Performance training modelThe underfit model -reacts even bigger fluctuations.underfit model -reacts even bigger fluctuations.Bias:Bias error introduced model oversimplification machine learning algorithm. can lead underfitting.supervised learning, underfitting happens model unable capture underlying pattern data. models usually high bias low variance. happens less amount data build accurate model try build linear model nonlinear data.Also, kinds models simple capture complex patterns data like Linear logistic regression.supervised learning, overfitting happens model captures noise along underlying pattern data.happens train model lot noisy dataset. models low bias high variance. models complex like Decision trees prone overfitting.Bias difference average prediction model correct value trying predict. Model high bias pays little attention training data oversimplifies model. always leads high error training test data.variance?Variance variability model prediction given data point value tells us spread data. Model high variance pays lot attention training data generalize data hasn‚Äôt seen . result, models perform well training data high error rates test data. (-fitting issue)Bias - Variance Trade-?model simple parameters may high bias low variance. hand, model large number parameters ‚Äôs going high variance low bias. , need find right/good balance without overfitting underfitting data.Name three types biases can occur samplingIn sampling process, three types biases, :‚Ä¢ Selection bias\n‚Ä¢ coverage bias\n‚Ä¢ Survivorship biasQuestion: understand Selection Bias? various types?Answer: Selection bias typically associated research doesn‚Äôt random selection participants. type error occurs researcher decides going studied. occasions, selection bias also referred selection effect.words, selection bias distortion statistical analysis results sample collecting method. selection bias taken account, conclusions made research study might accurate. Following various types selection bias:‚Ä¢ Sampling Bias ‚Äì systematic error resulting due non-random sample populace causing certain members less likely included others results biased sample.‚Ä¢ Time Interval ‚Äì trial might ended extreme value, usually due ethical reasons, extreme value likely reached variable variance, even though variables similar mean.‚Ä¢ Data ‚Äì Results specific data subsets selected supporting conclusion rejection bad data arbitrarily.‚Ä¢ Attrition ‚Äì Caused due attrition, .e.¬†loss participants, discounting trial subjects tests didn‚Äôt run completion.Discuss Decision Tree algorithm\ndecision tree popular supervised machine learning algorithm. mainly used Regression Classification. allows breaks dataset smaller subsets. decision tree can able handle categorical numerical data.Discuss Decision Tree algorithm\ndecision tree popular supervised machine learning algorithm. mainly used Regression Classification. allows breaks dataset smaller subsets. decision tree can able handle categorical numerical data.Prior probability likelihood?\nPrior probability proportion dependent variable data set likelihood probability classifying given observant presence variable.Prior probability likelihood?\nPrior probability proportion dependent variable data set likelihood probability classifying given observant presence variable.Explain Recommender Systems?\nsubclass information filtering techniques. helps predict preferences ratings users likely give product.Explain Recommender Systems?\nsubclass information filtering techniques. helps predict preferences ratings users likely give product.Question: Please explain Recommender Systems along application.\nAnswer: Recommender Systems subclass information filtering systems, meant predicting preferences ratings awarded user product.\napplication recommender system product recommendations section Amazon. section contains items based user‚Äôs search history past orders.Name three disadvantages using linear model\nThree disadvantages linear model :\n‚Ä¢ assumption linearity errors.\n‚Ä¢ can‚Äôt use model binary count outcomes\n‚Ä¢ plenty overfitting problems can‚Äôt solveName three disadvantages using linear model\nThree disadvantages linear model :\n‚Ä¢ assumption linearity errors.\n‚Ä¢ can‚Äôt use model binary count outcomes\n‚Ä¢ plenty overfitting problems can‚Äôt solveWhy need perform resampling?need perform resampling?Resampling done -given cases:\n‚Ä¢ Estimating accuracy sample statistics drawing randomly replacement set data point using subsets accessible data\n‚Ä¢ Substituting labels data points performing necessary tests\n‚Ä¢ Validating models using random subsetsList libraries Python used Data Analysis Scientific Computations.\nSciPy, Pandas, Matplotlib, NumPy, SciKit, SeabornWhat Power Analysis?\npower analysis integral part experimental design. helps determine sample size requires find effect given size cause specific level assurance. also allows deploy particular probability sample size constraint.Explain Collaborative filtering\nCollaborative filtering used search correct patterns collaborating viewpoints, multiple data sources, various agents.Discuss ‚ÄòNaive‚Äô Naive Bayes algorithm?\nNaive Bayes Algorithm model based Bayes Theorem. describes probability event. based prior knowledge conditions might related specific event.Linear Regression?\nLinear regression statistical programming method score variable ‚Äò‚Äô predicted score second variable ‚ÄòB‚Äô. B referred predictor variable criterion variable.State difference expected value mean value\nmany differences, terms used different contexts. Mean value generally referred discussing probability distribution whereas expected value referred context random variable.aim conducting /B Testing?\nAB testing used conduct random experiments two variables, B. goal testing method find changes web page maximize increase outcome strategy.Ensemble Learning?\nensemble method combining diverse set learners together improvise stability predictive power model. Two types Ensemble learning methods :Bagging\nBagging method helps implement similar learners small sample populations. helps make nearer predictions.\nBoosting\nBoosting iterative method allows adjust weight observation depends upon last classification. Boosting decreases bias error helps build strong predictive models.\n18. Explain Eigenvalue Eigenvector\nEigenvectors understanding linear transformations. Data scientist need calculate eigenvectors covariance matrix correlation. Eigenvalues directions along using specific linear transformation acts compressing, flipping, stretching.\nQuestion: Please explain Eigenvectors Eigenvalues.\nAnswer: Eigenvectors help understanding linear transformations. calculated typically correlation covariance matrix data analysis.\nwords, eigenvectors directions along particular linear transformation acts compressing, flipping, stretching.\nEigenvalues can understood either strengths transformation direction eigenvectors factors compressions happens.Define term cross-validation\nCross-validation validation technique evaluating outcomes statistical analysis generalize Independent dataset. method used backgrounds objective forecast, one needs estimate accurately model accomplish.\nQuestion: Can compare validation set test set?\nAnswer: validation set part training set used parameter selection well avoiding overfitting machine learning model developed. contrary, test set meant evaluating testing performance trained machine learning model.Define term cross-validation\nCross-validation validation technique evaluating outcomes statistical analysis generalize Independent dataset. method used backgrounds objective forecast, one needs estimate accurately model accomplish.\nQuestion: Can compare validation set test set?\nAnswer: validation set part training set used parameter selection well avoiding overfitting machine learning model developed. contrary, test set meant evaluating testing performance trained machine learning model.Explain steps Data analytics project\nfollowing important steps involved analytics project:\n‚Ä¢ Understand Business problem\n‚Ä¢ Explore data study carefully.\n‚Ä¢ Prepare data modeling finding missing values transforming variables.\n‚Ä¢ Start running model analyze Big data result.\n‚Ä¢ Validate model new data set.\n‚Ä¢ Implement model track result analyze performance model specific period.Explain steps Data analytics project\nfollowing important steps involved analytics project:\n‚Ä¢ Understand Business problem\n‚Ä¢ Explore data study carefully.\n‚Ä¢ Prepare data modeling finding missing values transforming variables.\n‚Ä¢ Start running model analyze Big data result.\n‚Ä¢ Validate model new data set.\n‚Ä¢ Implement model track result analyze performance model specific period.Question: mean cluster sampling systematic sampling?\nAnswer: studying target population spread throughout wide area becomes difficult applying simple random sampling becomes ineffective, technique cluster sampling used. cluster sample probability sample, sampling units collection cluster elements.\nFollowing technique systematic sampling, elements chosen ordered sampling frame. list advanced circular fashion. done way end list reached, progressed start, top, .Random Forest?\nRandom forest machine learning method helps perform types regression classification tasks. also used treating missing values outlier values.importance selection bias?\nSelection Bias occurs specific randomization achieved picking individuals groups data analyzed. suggests given sample exactly represent population intended analyzed.K-means clustering method?\nK-means clustering important unsupervised learning method. technique classifying data using certain set clusters called K clusters. deployed grouping find similarity data.Explain difference Data Science Data Analytics\nData Scientists need slice data extract valuable insights data analyst can apply real-world business scenarios. main difference two data scientists technical knowledge business analyst. Moreover, don‚Äôt need understanding business required data visualization.Explain p-value?\nconduct hypothesis test statistics, p-value allows determine strength results. numerical number 0 1. Based value help denote strength specific result.Define term deep learning\nDeep Learning subtype machine learning. concerned algorithms inspired structure called artificial neural networks (ANN).Explain method collect analyze data use social media predict weather condition.\ncan collect social media data using Facebook, twitter, Instagram‚Äôs API‚Äôs. example, tweeter, can construct feature tweet like tweeted date, retweets, list follower, etc. can use multivariate time series model predict weather condition.need update algorithm Data science?\nneed update algorithm following situation:\n‚Ä¢ want data model evolve data streams using infrastructure\n‚Ä¢ underlying data source changing non-stationarityWhat Normal Distribution\nnormal distribution set continuous variable spread across normal curve shape bell curve. can consider continuous probability distribution useful statistics. useful analyze variables relationships using normal distribution curve.language best text analytics? R Python?\nPython suitable text analytics consists rich library known pandas. allows use high-level data analysis tools data structures, R doesn‚Äôt offer feature.Explain benefits using statistics Data Scientists\nStatistics help Data scientist get better idea customer‚Äôs expectation. Using statistic method Data Scientists can get knowledge regarding consumer interest, behavior, engagement, retention, etc. also helps build powerful data models validate certain inferences predictions.Name various types Deep Learning Frameworks\n‚Ä¢ Pytorch\n‚Ä¢ Microsoft Cognitive Toolkit\n‚Ä¢ TensorFlow\n‚Ä¢ Caffe\n‚Ä¢ Chainer\n‚Ä¢ KerasExplain Data Cleansing essential method use maintain clean data\nDirty data often leads incorrect inside, can damage prospect organization. example, want run targeted marketing campaign. However, data incorrectly tell specific product -demand target audience; campaign fail.skewed Distribution & uniform distribution?\nSkewed distribution occurs data distributed one side plot whereas uniform distribution identified data spread equal range.underfitting occurs static model?\nUnderfitting occurs statistical model machine learning algorithm able capture underlying trend data.Name commonly used algorithms.\nFour commonly used algorithm Data scientist :\n‚Ä¢ Linear regression\n‚Ä¢ Logistic regression\n‚Ä¢ Random Forest\n‚Ä¢ KNNWhat precision?\nPrecision commonly used error metric n classification mechanism. range 0 1, 1 represents 100%univariate analysis?\nanalysis applied none attribute time known univariate analysis. Boxplot widely used, univariate model.overcome challenges findings?\norder, overcome challenges finding one need encourage discussion, Demonstrate leadership respecting different options.Explain cluster sampling technique Data science\ncluster sampling method used challenging study target population spread across, simple random sampling can‚Äôt applied.State difference Validation Set Test Set\nValidation set mostly considered part training set used parameter selection helps avoid overfitting model built.\nTest Set used testing evaluating performance trained machine learning model.Explain term Binomial Probability Formula?\n‚Äúbinomial distribution contains probabilities every possible success N trials independent events probability œÄ occurring.‚Äùrecall?\nrecall ratio true positive rate actual positive rate. ranges 0 1.Discuss normal distribution\nNormal distribution equally distributed mean, median mode equal.working data set, can select important variables? Explain\nFollowing methods variable selection can use:\n‚Ä¢ Remove correlated variables selecting important variables\n‚Ä¢ Use linear regression select variables depend p values.\n‚Ä¢ Use Backward, Forward Selection, Stepwise Selection\n‚Ä¢ Use Xgboost, Random Forest, plot variable importance chart.\n‚Ä¢ Measure information gain given set features select top n features accordingly.possible capture correlation continuous categorical variable?\nYes, can use analysis covariance technique capture association continuous categorical variables.Treating categorical variable continuous variable result better predictive model?\nYes, categorical value considered continuous variable variable ordinal nature. , better predictive model.\nQuestion:\nRecall: proportion actual positives identified correctly?\nTP / (TP + FN)\nPrecision: proportion positive identifications actually correct?\nTP / (TP + FP)\nQuestion:\nfalse positive incorrect identification absence condition absent.\nfalse negative incorrect identification absence condition actually present.\nQuestion: Please explain goal /B Testing.\nAnswer: /B Testing statistical hypothesis testing meant randomized experiment two variables, B. goal /B Testing maximize likelihood outcome interest identifying changes webpage.\nhighly reliable method finding best online marketing promotional strategies business, /B Testing can employed testing everything, ranging sales emails search ads website copy.Question: explain define number clusters clustering algorithm?\nAnswer: primary objective clustering group together similar identities way entities within group similar , groups remain different one another.\nGenerally, Within Sum Squares used explaining homogeneity within cluster. defining number clusters clustering algorithm, WSS plotted range pertaining number clusters. resultant graph known Elbow Curve.\nElbow Curve graph contains point represents point post aren‚Äôt decrements WSS. known bending point represents K K‚ÄìMeans.\nAlthough aforementioned widely-used approach, another important approach Hierarchical clustering. approach, dendrograms created first distinct groups identified .Question: Please explain Gradient Descent.\nAnswer: degree change output function relating changes made inputs known gradient. measures change weights respect change error. gradient can also comprehended slope function.\nGradient Descent refers escalating bottom valley. Simply, consider something opposed climbing hill. minimization algorithm meant minimizing given activation function.Question: Please enumerate various steps involved analytics project.\nAnswer: Following numerous steps involved analytics project:\n‚Ä¢ Understanding business problem\n‚Ä¢ Exploring data familiarizing \n‚Ä¢ Preparing data modeling means detecting outlier values, transforming variables, treating missing values, et cetera\n‚Ä¢ Running model analyzing result making appropriate changes modifications model (iterative step repeats best possible outcome gained)\n‚Ä¢ Validating model using new dataset\n‚Ä¢ Implementing model tracking result analyzing performance sameQuestion: outlier values treat ?\nAnswer: Outlier values, simply outliers, data points statistics don‚Äôt belong certain population. outlier value abnormal observation much different values belonging set.\nIdentification outlier values can done using univariate graphical analysis method. outlier values can assessed individually assessing large set outlier values require substitution either 99th 1st percentile values.\ntwo popular ways treating outlier values:\n1. change value can brought within range\n2. simply remove value\nNote: - extreme values outlier values.Discuss Artificial Neural Networks\nArtificial Neural networks (ANN) special set algorithms revolutionized machine learning. helps adapt according changing input. network generates best possible result without redesigning output criteria.Discuss Artificial Neural Networks\nArtificial Neural networks (ANN) special set algorithms revolutionized machine learning. helps adapt according changing input. network generates best possible result without redesigning output criteria.Back Propagation?\nBack-propagation essence neural net training. method tuning weights neural net depend upon error rate obtained previous epoch. Proper tuning helps reduce error rates make model reliable increasing generalization.Back Propagation?\nBack-propagation essence neural net training. method tuning weights neural net depend upon error rate obtained previous epoch. Proper tuning helps reduce error rates make model reliable increasing generalization.Explain Auto-EncoderAutoencoders learning networks. helps transform inputs outputs fewer numbers errors. means get output close input possible.Define Boltzmann Machine\nBoltzmann machines simple learning algorithm. helps discover features represent complex regularities training data. algorithm allows optimize weights quantity given problem.Define Boltzmann Machine\nBoltzmann machines simple learning algorithm. helps discover features represent complex regularities training data. algorithm allows optimize weights quantity given problem.reinforcement learning?\nReinforcement Learning learning mechanism map situations actions. end result help increase binary reward signal. method, learner told action take instead must discover action offers maximum reward. method based reward/penalty mechanism.reinforcement learning?\nReinforcement Learning learning mechanism map situations actions. end result help increase binary reward signal. method, learner told action take instead must discover action offers maximum reward. method based reward/penalty mechanism.Training-Validation-TestWe typically train model get evaluation metrics test data.ISLR:general, really care well method works training data. Rather, interested accuracy predictions obtain apply method previously unseen data. care ? Suppose interested developing algorithm predict stock‚Äôs price based previous stock returns. can train method using stock returns past 6 months. don‚Äôt really care well method predicts last week‚Äôs stock price. instead care well predict tomorrow‚Äôs price next month‚Äôs price.general, really care well method works training data. Rather, interested accuracy predictions obtain apply method previously unseen data. care ? Suppose interested developing algorithm predict stock‚Äôs price based previous stock returns. can train method using stock returns past 6 months. don‚Äôt really care well method predicts last week‚Äôs stock price. instead care well predict tomorrow‚Äôs price next month‚Äôs price.can use patients data train statistical learning method predict risk diabetes based clinical measurements. practice, want method accurately predict diabetes risk future patients based clinical measurements. interested whether method accurately predicts diabetes risk patients used train model, since already know patients diabetes.can use patients data train statistical learning method predict risk diabetes based clinical measurements. practice, want method accurately predict diabetes risk future patients based clinical measurements. interested whether method accurately predicts diabetes risk patients used train model, since already know patients diabetes.want choose method gives lowest test MSE, opposed lowest training MSE.want choose method gives lowest test MSE, opposed lowest training MSE.problem many statistical methods specifically estimate coefficients minimize training set MSE. methods, training set MSE can quite small, test MSE often much larger.problem many statistical methods specifically estimate coefficients minimize training set MSE. methods, training set MSE can quite small, test MSE often much larger.R-squaredIn simple linear regression \\(r^2 = R^2\\)ISLR:number near 0 indicates regression explain much variability response; might occur linear model wrong, error variance œÉ2 high, .number near 0 indicates regression explain much variability response; might occur linear model wrong, error variance œÉ2 high, .can still challenging determine good R2 value, general, depend application.can still challenging determine good R2 value, general, depend application.certain problems physics, may know data truly comes linear model small residual error. case, expect see R2 value extremely close 1, substantially smaller R2 value might indicate serious problem experiment data generated.certain problems physics, may know data truly comes linear model small residual error. case, expect see R2 value extremely close 1, substantially smaller R2 value might indicate serious problem experiment data generated.hand, typical applications biology, psychology, marketing, domains, linear model best extremely rough approximation data, residual errors due unmeasured factors often large. setting, expect small proportion variance response explained predictor, \\(R^2\\) value well 0.1 might realistic!hand, typical applications biology, psychology, marketing, domains, linear model best extremely rough approximation data, residual errors due unmeasured factors often large. setting, expect small proportion variance response explained predictor, \\(R^2\\) value well 0.1 might realistic!F-TestTesting whether regression coefficients zero, .e.¬†\n\\(H_0 :Œ≤_1 = Œ≤_2 =¬∑¬∑¬∑=Œ≤_p = 0\\)\\(Ha :\\) least one \\(Œ≤_j\\) non-zero.","code":""},{"path":"machine-learning-fundamentals.html","id":"bootstrapping","chapter":"2 Machine Learning Fundamentals","heading":"2.1.2 Bootstrapping","text":"","code":""},{"path":"machine-learning-fundamentals.html","id":"jack-knife","chapter":"2 Machine Learning Fundamentals","heading":"2.1.2.1 Jack-knife","text":"‚Ä¢ jackknife tool estimating standard errors bias estimators‚Ä¢ name suggests, jackknife small, handy tool; contrast \nbootstrap, moral equivalent giant workshop full tools‚Ä¢ jackknife bootstrap involve re-sampling data; , repeatedly creating new data sets original dataThe jackknife deletes observation calculates estimate based remaining n ‚àí 1 ‚Ä¢ uses collection estimates things like estimate bias standard error‚Ä¢ Note estimating bias standard error needed things like sample means, know unbiased estimates population means standard errors areIt shown jackknife linear approximation bootstrap‚Ä¢ Generally use jackknife sample quantiles like median; shown poor propertiesThe bootstrap‚Ä¢ bootstrap tremendously useful tool constructing confidence intervals calculating standard errors difficult statistics\n‚Ä¢ example, one derive confidence interval median? ‚Ä¢ bootstrap procedure follows called bootstrap principleSuppose statistic estimates population parameter, don‚Äôt know sampling distribution\n‚Ä¢ bootstrap principle suggests using distribution defined data approximate sampling distribution\n‚Ä¢ practice, bootstrap principle always carried using simulation\n‚Ä¢ general procedure follows first simulating complete data sets observed data replacement\n‚Ä¢ approximately drawing sampling distribution statistic, least far data able approximate true population distribution\n‚Ä¢ Calculate statistic simulated data set\n‚Ä¢ Use simulated statistics either define confidence interval take standard deviation calculate standard error\nExample\n‚Ä¢ Consider , data set 630 measurements gray matter volume workers lead manufacturing plant\n‚Ä¢ median gray matter volume around 589 cubic centimeters\n‚Ä¢ want confidence interval median measurements\n‚Ä¢ Bootstrap procedure calculating median data set n observations\n. Sample n observations replacement observed data resulting one simulated complete data set\nii. Take median simulated data set\niii. Repeat two steps B times, resulting B simulated medians\niv. medians approximately draws sampling distribution \nmedian n observations; therefore can\n‚Ä¢ Draw histogram \n‚Ä¢ Calculate standard deviation estimate standard error median ‚Ä¢ Take 2.5th 97.5th percentiles confidence interval medianSummary\n‚Ä¢ bootstrap non-parametric\n‚Ä¢ However, theoretical arguments proving validity bootstrap rely\nlarge samples\n‚Ä¢ Better percentile bootstrap confidence intervals correct bias\n‚Ä¢ lots variations bootstrap procedures; book ‚ÄúIntroduction Bootstrap‚Äù Efron Tibshirani great place start bootstrap jackknife information","code":""},{"path":"machine-learning-fundamentals.html","id":"classification-models-evaluation","chapter":"2 Machine Learning Fundamentals","heading":"2.1.3 Classification Models: Evaluation","text":"medium storyGoogle developers","code":""},{"path":"machine-learning-fundamentals.html","id":"thresholding","chapter":"2 Machine Learning Fundamentals","heading":"2.1.3.1 Thresholding","text":"Logistic regression returns probability. can use returned probability ‚Äú‚Äù (example, probability user click ad 0.00023) convert returned probability binary value (example, email spam).logistic regression model returns 0.9995 particular email message predicting likely spam. Conversely, another email message prediction score 0.0003 logistic regression model likely spam.However, email message prediction score 0.6? order map logistic regression value binary category, must define classification threshold (also called decision threshold).value threshold indicates ‚Äúspam‚Äù; value indicates ‚Äúspam.‚Äù tempting assume classification threshold always 0.5, thresholds problem-dependent, therefore values must tune.Note: ‚ÄúTuning‚Äù threshold logistic regression different tuning hyperparameters learning rate. Part choosing threshold assessing much ‚Äôll suffer making mistake. example, mistakenly labeling non-spam message spam bad. However, mistakenly labeling spam message non-spam unpleasant, hardly end job.","code":""},{"path":"machine-learning-fundamentals.html","id":"confusion-matrix","chapter":"2 Machine Learning Fundamentals","heading":"2.1.3.2 Confusion Matrix","text":"True Positive: Model predicted positive true.True negative: Model predicted negative true.False positive (Type 1 Error): Model predicted positive false.False negative (Type 2 Error): Model predicted negative true.False Positive Rate (FPR):False Positive Rate ratio false positive predictions total number actual negatives. measures rate model incorrectly predicts positive class among instances actually negative.\\(FPR = \\frac{FP}{TN + FP}\\)True Positive Rate (TPR), Sensitivity, Recall:True Positive Rate ratio true positive predictions total number actual positives. measures ability model correctly predict positive class among instances actually positive.\\(Recall (TPR) = \\frac{TP}{TP + FN}\\)Accuracy:represents ratio correctly predicted instances total number instances. accuracy metric suitable balanced datasets classes evenly distributed. calculated using following formula:\\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)Accuracy provides general sense well model performing across classes. easy understand interpret, making commonly used metric, especially classes balanced.However, accuracy may ideal metric situations class distribution imbalanced. imbalanced datasets, one class significantly outnumbers , high accuracy might achieved simply predicting majority class. cases, metrics like precision, recall, F1 score, area receiver operating characteristic (ROC-AUC) curve may informative.Precision:Precision ratio true positive predictions total number positive predictions made model. represents accuracy positive predictions made model.\\(Precision = \\frac{TP}{TP + FP}\\)F1 Measure:F1 score metric commonly used binary classification provide balance precision recall. harmonic mean precision recall, combining measures single value. F1 score particularly useful uneven class distribution false positives false negatives important considerations.F1 score useful situations achieving balance precision recall important, penalizes models significant imbalance two metrics.\\(F1 score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\)","code":""},{"path":"machine-learning-fundamentals.html","id":"in-marketing","chapter":"2 Machine Learning Fundamentals","heading":"2.1.3.3 In Marketing","text":"Precision:marketing, precision valuable cost impact associated false positives (incorrectly identifying non-lookalike lookalike) high. example, targeting non-lookalike marketing campaign significant costs, want minimize false positives.Recall:marketing, recall important want ensure missing potential opportunities (actual lookalikes). missing true lookalike high cost lost opportunity, want maximize recall.","code":""},{"path":"machine-learning.html","id":"machine-learning","chapter":"3 Machine Learning","heading":"3 Machine Learning","text":"Different machine learning algorithms suitable various types tasks, binary classification, multi-class classification, predicting continuous outcomes. commonly used algorithms task:","code":""},{"path":"machine-learning.html","id":"ml-algorithms-intro","chapter":"3 Machine Learning","heading":"3.1 ML Algorithms Intro","text":"","code":""},{"path":"machine-learning.html","id":"binary-classification","chapter":"3 Machine Learning","heading":"3.1.1 Binary Classification:","text":"Logistic Regression:\nLogistic Regression simple widely used algorithm binary classification tasks. models probability instance belongs particular class.\nLogistic Regression simple widely used algorithm binary classification tasks. models probability instance belongs particular class.Support Vector Machines (SVM):\nSVM effective binary classification. finds hyperplane best separates data two classes.\nSVM effective binary classification. finds hyperplane best separates data two classes.Random Forest:\nRandom Forest ensemble learning algorithm performs well binary multi-class classification tasks. builds multiple decision trees combines predictions.\nRandom Forest ensemble learning algorithm performs well binary multi-class classification tasks. builds multiple decision trees combines predictions.Gradient Boosting (e.g., XGBoost, LightGBM):\nGradient Boosting algorithms powerful binary classification tasks. build trees sequentially, tree correcting errors previous one.\nGradient Boosting algorithms powerful binary classification tasks. build trees sequentially, tree correcting errors previous one.Neural Networks:\nNeural networks, especially architectures like feedforward neural networks, can used binary classification tasks. particularly effective complex, non-linear relationships.\nNeural networks, especially architectures like feedforward neural networks, can used binary classification tasks. particularly effective complex, non-linear relationships.","code":""},{"path":"machine-learning.html","id":"multi-class-classification","chapter":"3 Machine Learning","heading":"3.1.2 Multi-Class Classification:","text":"Logistic Regression (One-vs-):\nLogistic Regression can extended handle multi-class classification training multiple binary classifiers (one class) one-vs-fashion.\nLogistic Regression can extended handle multi-class classification training multiple binary classifiers (one class) one-vs-fashion.Multinomial Naive Bayes:\nNaive Bayes can extended handle multiple classes, multinomial variant commonly used text classification tasks.\nNaive Bayes can extended handle multiple classes, multinomial variant commonly used text classification tasks.Random Forest:\nRandom Forest can handle multi-class classification naturally. builds multiple decision trees, final prediction based voting across classes.\nRandom Forest can handle multi-class classification naturally. builds multiple decision trees, final prediction based voting across classes.Gradient Boosting (e.g., XGBoost, LightGBM):\nGradient Boosting algorithms can handle multi-class classification tasks. build series decision trees, one correcting errors ensemble.\nGradient Boosting algorithms can handle multi-class classification tasks. build series decision trees, one correcting errors ensemble.K-Nearest Neighbors (KNN):\nKNN can used multi-class classification assigning class label common among k nearest neighbors.\nKNN can used multi-class classification assigning class label common among k nearest neighbors.","code":""},{"path":"machine-learning.html","id":"continuous-outcome-regression","chapter":"3 Machine Learning","heading":"3.1.3 Continuous Outcome (Regression):","text":"Linear Regression:\nLinear Regression basic widely used algorithm predicting continuous outcomes. models relationship features target variable linear equation.\nLinear Regression basic widely used algorithm predicting continuous outcomes. models relationship features target variable linear equation.Decision Trees Regression:\nDecision trees can used regression tasks predicting average value target variable leaf node.\nDecision trees can used regression tasks predicting average value target variable leaf node.Random Forest Regression:\nRandom Forest can applied regression tasks aggregating predictions multiple decision trees.\nRandom Forest can applied regression tasks aggregating predictions multiple decision trees.Gradient Boosting Regression (e.g., XGBoost, LightGBM):\nGradient Boosting algorithms can used regression tasks. build decision trees sequentially, one correcting errors ensemble.\nGradient Boosting algorithms can used regression tasks. build decision trees sequentially, one correcting errors ensemble.Support Vector Machines (SVR):\nSupport Vector Machines can used regression tasks finding hyperplane best fits data.\nSupport Vector Machines can used regression tasks finding hyperplane best fits data.just examples, choice algorithm depends factors size nature dataset, relationship features target variables, computational considerations. ‚Äôs often good practice experiment multiple algorithms choose one performs best specific task.Several machine learning algorithms popular widely used various applications. popularity algorithm often depends nature problem characteristics data. popular machine learning algorithms:Linear Regression:\nUsed predicting continuous outcome based one predictor features. ‚Äôs widely used regression analysis.\nUsed predicting continuous outcome based one predictor features. ‚Äôs widely used regression analysis.Logistic Regression:\nUsed binary classification problems. models probability given instance belongs particular class.\nUsed binary classification problems. models probability given instance belongs particular class.Decision Trees:\ntree-like model decisions, node represents test attribute, branch represents outcome test, leaf node represents class label.\ntree-like model decisions, node represents test attribute, branch represents outcome test, leaf node represents class label.Random Forest:\nensemble learning method constructs multitude decision trees training outputs mode classes classification mean prediction regression.\nensemble learning method constructs multitude decision trees training outputs mode classes classification mean prediction regression.Support Vector Machines (SVM):\nsupervised machine learning algorithm can used classification regression tasks. finds hyperplane best separates data classes.\nsupervised machine learning algorithm can used classification regression tasks. finds hyperplane best separates data classes.K-Nearest Neighbors (KNN):\nsimple, instance-based learning algorithm object classified neighbors. assigns class label based majority class k nearest neighbors.\nsimple, instance-based learning algorithm object classified neighbors. assigns class label based majority class k nearest neighbors.K-Means Clustering:\nclustering algorithm partitions data k clusters based similarity. ‚Äôs commonly used unsupervised learning tasks.\nclustering algorithm partitions data k clusters based similarity. ‚Äôs commonly used unsupervised learning tasks.Naive Bayes:\nprobabilistic algorithm based Bayes‚Äô theorem particularly suited classification tasks. assumes features conditionally independent given class.\nprobabilistic algorithm based Bayes‚Äô theorem particularly suited classification tasks. assumes features conditionally independent given class.Gradient Boosting (e.g., XGBoost, LightGBM):\nensemble learning technique weak models (typically decision trees) trained sequentially, new model corrects errors previous ones.\nensemble learning technique weak models (typically decision trees) trained sequentially, new model corrects errors previous ones.Neural Networks (Deep Learning):\nArtificial neural networks inspired structure function human brain. Deep learning models, feedforward neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), achieved remarkable success various tasks.\nArtificial neural networks inspired structure function human brain. Deep learning models, feedforward neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), achieved remarkable success various tasks.algorithms cover range tasks, including regression, classification, clustering, . choice algorithm depends specific problem hand characteristics data. Often, combination algorithms ensemble methods used achieve better performance.","code":""},{"path":"machine-learning.html","id":"random-forest-vs-decision-trees","chapter":"3 Machine Learning","heading":"3.1.4 Random Forest vs Decision Trees","text":"Decision Trees Random Forest machine learning algorithms, Random Forest ensemble learning method builds Decision Trees. key differences Decision Trees Random Forest:","code":""},{"path":"machine-learning.html","id":"decision-trees","chapter":"3 Machine Learning","heading":"3.1.4.1 Decision Trees:","text":"Single Model:\nDecision Tree single model recursively splits dataset based significant feature node.\nDecision Tree single model recursively splits dataset based significant feature node.Vulnerability Overfitting:\nDecision Trees prone overfitting, especially tree deep captures noise training data.\nDecision Trees prone overfitting, especially tree deep captures noise training data.High Variance:\nDue tendency overfit, Decision Trees high variance, meaning can sensitive small changes training data.\nDue tendency overfit, Decision Trees high variance, meaning can sensitive small changes training data.Bias-Variance Tradeoff:\nDecision Trees example model high bias (simple) high variance (complex). Finding right level complexity crucial.\nDecision Trees example model high bias (simple) high variance (complex). Finding right level complexity crucial.Interpretability:\nDecision Trees generally interpretable, ‚Äôs easier understand decision-making process node.\nDecision Trees generally interpretable, ‚Äôs easier understand decision-making process node.","code":""},{"path":"machine-learning.html","id":"random-forest","chapter":"3 Machine Learning","heading":"3.1.4.2 Random Forest:","text":"Ensemble Method:\nRandom Forest ensemble method builds multiple Decision Trees combines predictions. tree trained random subset data features.\nRandom Forest ensemble method builds multiple Decision Trees combines predictions. tree trained random subset data features.Reduced Overfitting:\naggregating predictions multiple trees, Random Forest reduces overfitting compared single Decision Tree. achieves better balance bias variance.\naggregating predictions multiple trees, Random Forest reduces overfitting compared single Decision Tree. achieves better balance bias variance.Improved Generalization:\nRandom Forest improves generalization performance creating diverse trees capture different aspects data. final prediction average voting mechanism.\nRandom Forest improves generalization performance creating diverse trees capture different aspects data. final prediction average voting mechanism.Robustness:\nRandom Forest robust outliers noisy data compared single Decision Tree ensemble nature helps filter noise.\nRandom Forest robust outliers noisy data compared single Decision Tree ensemble nature helps filter noise.Automatic Feature Selection:\nRandom Forest provides form automatic feature selection considering random subset features split tree.\nRandom Forest provides form automatic feature selection considering random subset features split tree.Higher Computational Cost:\nBuilding multiple trees combining predictions increases computational cost compared single Decision Tree.\nBuilding multiple trees combining predictions increases computational cost compared single Decision Tree.summary, Decision Trees simple interpretable, prone overfitting. Random Forest addresses limitation constructing ensemble trees, leading better generalization robustness cost increased computational complexity. Random Forest powerful widely used algorithm, especially tasks high accuracy robustness important.","code":""},{"path":"machine-learning.html","id":"random-forest-vs-gradient-boosting","chapter":"3 Machine Learning","heading":"3.1.5 Random Forest vs Gradient Boosting","text":"Random Forest Gradient Boosting ensemble learning techniques, key differences:","code":""},{"path":"machine-learning.html","id":"random-forest-1","chapter":"3 Machine Learning","heading":"3.1.5.1 Random Forest:","text":"Ensemble Type:\nRandom Forest ensemble decision trees. builds multiple decision trees independently combines predictions averaging (regression) voting (classification).\nRandom Forest ensemble decision trees. builds multiple decision trees independently combines predictions averaging (regression) voting (classification).Parallel Training:\nTrees Random Forest can trained independently parallel, making computationally efficient. tree constructed based random subset data.\nTrees Random Forest can trained independently parallel, making computationally efficient. tree constructed based random subset data.Feature Subset Split:\nsplit tree, random subset features considered. introduces element randomness, reducing risk overfitting making model robust.\nsplit tree, random subset features considered. introduces element randomness, reducing risk overfitting making model robust.Voting Mechanism:\nclassification tasks, final prediction determined majority vote individual trees. regression tasks, final prediction average predictions trees.\nclassification tasks, final prediction determined majority vote individual trees. regression tasks, final prediction average predictions trees.Less Prone Overfitting:\nRandom Forest less prone overfitting compared individual decision trees, making robust model.\nRandom Forest less prone overfitting compared individual decision trees, making robust model.","code":""},{"path":"machine-learning.html","id":"gradient-boosting","chapter":"3 Machine Learning","heading":"3.1.5.2 Gradient Boosting:","text":"Ensemble Type:\nGradient Boosting also ensemble decision trees, unlike Random Forest, builds trees sequentially, tree correcting errors previous one.\nGradient Boosting also ensemble decision trees, unlike Random Forest, builds trees sequentially, tree correcting errors previous one.Sequential Training:\nTrees trained sequentially, subsequent tree focuses minimizing errors made combined ensemble previous trees.\nTrees trained sequentially, subsequent tree focuses minimizing errors made combined ensemble previous trees.Emphasis Misclassifications:\nGradient Boosting places emphasis correcting mistakes ensemble. tree fitted residuals (errors) combined model.\nGradient Boosting places emphasis correcting mistakes ensemble. tree fitted residuals (errors) combined model.Weighted Voting:\nstep, predictions trees combined weights, weights determined model‚Äôs performance training data.\nstep, predictions trees combined weights, weights determined model‚Äôs performance training data.Potential Overfitting:\nGradient Boosting potential overfit training data, especially model complex learning rate high.\nGradient Boosting potential overfit training data, especially model complex learning rate high.Sensitive Hyperparameters:\nperformance Gradient Boosting models sensitive hyperparameter tuning compared Random Forest.\nperformance Gradient Boosting models sensitive hyperparameter tuning compared Random Forest.","code":""},{"path":"machine-learning.html","id":"overall-considerations","chapter":"3 Machine Learning","heading":"3.1.6 Overall Considerations:","text":"Parallelization:\nRandom Forest can easily parallelized, making suitable distributed computing environments.\nGradient Boosting, sequential process, easily parallelized.\nRandom Forest can easily parallelized, making suitable distributed computing environments.Gradient Boosting, sequential process, easily parallelized.Hyperparameter Tuning:\nGradient Boosting typically requires careful hyperparameter tuning Random Forest.\nGradient Boosting typically requires careful hyperparameter tuning Random Forest.Performance:\nmodels powerful widely used, performance can vary depending characteristics dataset.\nmodels powerful widely used, performance can vary depending characteristics dataset.summary, Random Forest Gradient Boosting ensemble methods based decision trees, differ construction, training process, emphasis correcting errors. choice depends specific characteristics data goals modeling task.","code":""},{"path":"machine-learning.html","id":"ml-libraries-in-python","chapter":"3 Machine Learning","heading":"3.2 ML Libraries in Python","text":"Several libraries widely used machine learning addition scikit-learn. popular ones:TensorFlow:\nDeveloped Google Brain, TensorFlow open-source machine learning library widely used deep learning applications. provides comprehensive set tools community support.\nDeveloped Google Brain, TensorFlow open-source machine learning library widely used deep learning applications. provides comprehensive set tools community support.PyTorch:\nPyTorch, developed Facebook‚Äôs AI Research lab (FAIR), another popular deep learning library. known dynamic computational graph, making flexible research experimentation.\nPyTorch, developed Facebook‚Äôs AI Research lab (FAIR), another popular deep learning library. known dynamic computational graph, making flexible research experimentation.Keras:\nKeras can used high-level neural networks API TensorFlow, now also integrated TensorFlow official high-level API. provides simple user-friendly interface building neural networks.\nKeras can used high-level neural networks API TensorFlow, now also integrated TensorFlow official high-level API. provides simple user-friendly interface building neural networks.XGBoost:\nXGBoost efficient scalable implementation gradient boosting. widely used structured/tabular data known high performance Kaggle competitions.\nXGBoost efficient scalable implementation gradient boosting. widely used structured/tabular data known high performance Kaggle competitions.LightGBM:\nLightGBM gradient boosting framework developed Microsoft. designed distributed efficient training large-scale datasets particularly useful categorical features.\nLightGBM gradient boosting framework developed Microsoft. designed distributed efficient training large-scale datasets particularly useful categorical features.CatBoost:\nCatBoost gradient boosting library designed handle categorical features efficiently. developed Yandex known ease use.\nCatBoost gradient boosting library designed handle categorical features efficiently. developed Yandex known ease use.Pandas:\nPandas specifically machine learning library, essential library data manipulation analysis. often used preprocessing phase machine learning workflows.\nPandas specifically machine learning library, essential library data manipulation analysis. often used preprocessing phase machine learning workflows.NumPy SciPy:\nlibraries fundamental scientific computing Python. NumPy provides support large, multi-dimensional arrays matrices, SciPy builds NumPy provides additional functionality optimization, signal processing, .\nlibraries fundamental scientific computing Python. NumPy provides support large, multi-dimensional arrays matrices, SciPy builds NumPy provides additional functionality optimization, signal processing, .NLTK SpaCy:\nNatural Language Toolkit (NLTK) SpaCy libraries used natural language processing (NLP). provide tools tasks tokenization, part--speech tagging, named entity recognition.\nNatural Language Toolkit (NLTK) SpaCy libraries used natural language processing (NLP). provide tools tasks tokenization, part--speech tagging, named entity recognition.Statsmodels:\nStatsmodels library estimating testing statistical models. commonly used statistical analysis hypothesis testing.\nStatsmodels library estimating testing statistical models. commonly used statistical analysis hypothesis testing.libraries cover broad range machine learning tasks, traditional machine learning algorithms deep learning specialized tools tasks like natural language processing. choice library often depends specific requirements machine learning project.","code":""},{"path":"machine-learning.html","id":"big-data-solutions","chapter":"3 Machine Learning","heading":"3.2.1 Big data solutions","text":"dealing big data machine learning, specialized libraries frameworks can handle distributed computing parallel processing become essential. popular libraries frameworks big data machine learning:Apache Spark MLlib:\nSpark MLlib part Apache Spark ecosystem provides scalable machine learning libraries Spark. includes algorithms classification, regression, clustering, collaborative filtering, . Spark‚Äôs distributed computing capabilities make well-suited big data processing.\nSpark MLlib part Apache Spark ecosystem provides scalable machine learning libraries Spark. includes algorithms classification, regression, clustering, collaborative filtering, . Spark‚Äôs distributed computing capabilities make well-suited big data processing.Dask-ML:\nDask parallel computing library Python integrates popular libraries like NumPy, Pandas, Scikit-Learn. Dask-ML extends Scikit-Learn support larger--memory computations using parallel processing.\nDask parallel computing library Python integrates popular libraries like NumPy, Pandas, Scikit-Learn. Dask-ML extends Scikit-Learn support larger--memory computations using parallel processing.H2O.ai:\nH2O.ai offers open-source machine learning platform includes H2O-3, distributed machine learning library. H2O-3 supports variety machine learning algorithms designed scale horizontally.\nH2O.ai offers open-source machine learning platform includes H2O-3, distributed machine learning library. H2O-3 supports variety machine learning algorithms designed scale horizontally.MLlib Apache Flink:\nApache Flink stream processing framework, MLlib machine learning library. allows build machine learning pipelines streaming environment, making suitable real-time analytics big data.\nApache Flink stream processing framework, MLlib machine learning library. allows build machine learning pipelines streaming environment, making suitable real-time analytics big data.PySpark (Python API Apache Spark):\nPySpark Python API Apache Spark. enables Python developers use Spark distributed data processing machine learning tasks. PySpark‚Äôs MLlib machine learning library used within PySpark ecosystem.\nPySpark Python API Apache Spark. enables Python developers use Spark distributed data processing machine learning tasks. PySpark‚Äôs MLlib machine learning library used within PySpark ecosystem.Scikit-Spark (formerly known BigML):\nScikit-Spark extension Scikit-Learn allows distribute machine learning computations across cluster. ‚Äôs built top Apache Spark designed handle large datasets.\nScikit-Spark extension Scikit-Learn allows distribute machine learning computations across cluster. ‚Äôs built top Apache Spark designed handle large datasets.TensorFlow Extended (TFX):\nTFX end--end platform deploying production-ready machine learning models scale. built Google includes components data validation, transformation, training, serving.\nTFX end--end platform deploying production-ready machine learning models scale. built Google includes components data validation, transformation, training, serving.Apache Mahout:\nApache Mahout open-source project provides scalable machine learning algorithms. designed work distributed data processing frameworks like Apache Hadoop.\nApache Mahout open-source project provides scalable machine learning algorithms. designed work distributed data processing frameworks like Apache Hadoop.KNIME Analytics Platform:\nKNIME open-source platform allows data scientists visually design, execute, reuse machine learning workflows. supports big data processing integration Apache Spark Hadoop.\nKNIME open-source platform allows data scientists visually design, execute, reuse machine learning workflows. supports big data processing integration Apache Spark Hadoop.Cerebro:\nCerebro Python library distributed machine learning Apache Spark. designed provide interface similar Scikit-Learn distributed computing.\nCerebro Python library distributed machine learning Apache Spark. designed provide interface similar Scikit-Learn distributed computing.working big data, choice library framework depends specific requirements project, characteristics data, infrastructure available. Apache Spark particularly popular choice due widespread adoption big data community.","code":""},{"path":"machine-learning.html","id":"databricks","chapter":"3 Machine Learning","heading":"3.2.2 Databricks","text":"Databricks cloud-based platform built top Apache Spark, provides collaborative environment big data analytics machine learning. Databricks, access various machine learning libraries integrate seamlessly Apache Spark. key machine learning libraries commonly used Databricks:MLlib (Spark MLlib):\nApache Spark MLlib native machine learning library Spark. provides scalable set machine learning algorithms tools, making fundamental choice machine learning tasks Databricks.\nApache Spark MLlib native machine learning library Spark. provides scalable set machine learning algorithms tools, making fundamental choice machine learning tasks Databricks.Scikit-learn:\nScikit-learn popular machine learning library Python. ‚Äôs native Spark, can use Databricks notebooks perform machine learning tasks smaller datasets fit memory.\nScikit-learn popular machine learning library Python. ‚Äôs native Spark, can use Databricks notebooks perform machine learning tasks smaller datasets fit memory.XGBoost LightGBM:\nXGBoost LightGBM gradient boosting libraries widely used machine learning tasks. can integrated Databricks boosting algorithms large-scale datasets.\nXGBoost LightGBM gradient boosting libraries widely used machine learning tasks. can integrated Databricks boosting algorithms large-scale datasets.TensorFlow PyTorch:\nTensorFlow PyTorch popular deep learning frameworks. Databricks provides support frameworks, allowing build train deep learning models using distributed computing capabilities.\nTensorFlow PyTorch popular deep learning frameworks. Databricks provides support frameworks, allowing build train deep learning models using distributed computing capabilities.Horovod:\nHorovod distributed deep learning training framework works TensorFlow, PyTorch, Apache MXNet. allows scale deep learning training across multiple nodes Databricks cluster.\nHorovod distributed deep learning training framework works TensorFlow, PyTorch, Apache MXNet. allows scale deep learning training across multiple nodes Databricks cluster.Koalas:\nKoalas Pandas API Apache Spark, making easier data scientists familiar Pandas work large-scale datasets using Spark infrastructure. ‚Äôs machine learning library can useful data preprocessing exploration.\nKoalas Pandas API Apache Spark, making easier data scientists familiar Pandas work large-scale datasets using Spark infrastructure. ‚Äôs machine learning library can useful data preprocessing exploration.Delta Lake:\nmachine learning library, Delta Lake storage layer brings ACID transactions Apache Spark big data workloads. can used conjunction machine learning workflows manage version large datasets.\nmachine learning library, Delta Lake storage layer brings ACID transactions Apache Spark big data workloads. can used conjunction machine learning workflows manage version large datasets.MLflow:\nMLflow open-source platform managing end--end machine learning lifecycle. provides tools tracking experiments, packaging code reproducible runs, sharing deploying models. MLflow can easily integrated Databricks.\nMLflow open-source platform managing end--end machine learning lifecycle. provides tools tracking experiments, packaging code reproducible runs, sharing deploying models. MLflow can easily integrated Databricks.working Databricks, ‚Äôs common leverage MLlib distributed machine learning tasks use external libraries like Scikit-learn, TensorFlow, PyTorch specific algorithms deep learning workloads. Additionally, Databricks integrates MLflow streamline machine learning workflow.","code":""},{"path":"machine-learning.html","id":"naive-bayes","chapter":"3 Machine Learning","heading":"3.3 Naive Bayes","text":"Naive Bayes models group extremely fast simple classification algorithms often suitable high-dimensional datasets. fast tunable parameters, end useful quick--dirty baseline classification problem.","code":""},{"path":"machine-learning.html","id":"bayesian-classification","chapter":"3 Machine Learning","heading":"3.3.1 Bayesian Classification","text":"rely Bayes‚Äôs theorem, equation describing relationship conditional probabilities statistical quantities. Bayesian classification, ‚Äôre interested finding probability label given observed featuresGaussian Naive BayesPerhaps easiest naive Bayes classifier understand Gaussian naive Bayes. classifier, assumption data label drawn simple Gaussian distribution. Imagine following data:Use Naive BayesBecause naive Bayesian classifiers make stringent assumptions data, generally perform well complicated model. said, several advantages:‚Ä¢ extremely fast training prediction\n‚Ä¢ provide straightforward probabilistic prediction\n‚Ä¢ often easily interpretable\n‚Ä¢ () tunable parametersThese advantages mean naive Bayesian classifier often good choice initial baseline classification. performs suitably, congratulations: fast, interpretable classifier problem. perform well, can begin exploring sophisticated models, baseline knowledge well perform.Naive Bayes classifiers tend perform especially well one following situations:\n‚Ä¢ naive assumptions actually match data (rare practice)\n‚Ä¢ well-separated categories, model complexity less important\n‚Ä¢ high-dimensional data, model complexity less important","code":""}]
