[["index.html", "Projects Repository Preface", " Projects Repository Davut Ayan 2023-12-17 Preface Hello there! üëã As a devoted explorer navigating the expansive realm of machine learning, I am delighted to present my personal repository‚Äîa virtual haven that houses my notes, musings, and sample projects sourced from a diverse array of blogs, books, and practical encounters. This curated collection serves as a mosaic of insights, with some of the codes and notes thoughtfully extracted from publicly available machine learning blogs. Each project within this repository is a testament to my ongoing quest for understanding, meticulously pieced together from the rich tapestry of the digital knowledge landscape. Whether you are a fellow enthusiast, a curious mind, or a seasoned practitioner, I extend an invitation to explore the codebase, delve into the concepts, and perhaps find inspiration for your own machine learning journey. This repository is not merely a repository of algorithms and snippets; it is a reflection of my commitment, curiosity, and enthusiasm for the ever-evolving field of machine learning. I encourage you to engage, share your thoughts, or even collaborate on this journey. Let‚Äôs celebrate the collaborative spirit of the machine learning community and together, embrace the boundless opportunities that arise from the fusion of code, data, and the collective wisdom of publicly available resources. Happy exploration! "],["projects.html", "Chapter 1 Projects 1.1 Elastic Net Model 1.2 Survival Analysis", " Chapter 1 Projects DataTab Statistics Tutorials 1.1 Elastic Net Model Elastic Net is a regularization technique that combines both L1 (Lasso) and L2 (Ridge) regularization penalties in a linear regression model. This technique is commonly used in machine learning, especially when dealing with high-dimensional datasets or situations where some of the features are highly correlated. In elastic net regularization, the objective function is a combination of the L1 and L2 regularization terms along with the linear regression loss. The regularization strength is controlled by two hyperparameters, often denoted as Œ± Œ± (alpha) and Œª Œª (lambda): Œ± Œ± controls the mixing between L1 and L2 regularization. When Œ± = 0 Œ±=0, it is equivalent to Ridge regression, and when Œ± = 1 Œ±=1, it is equivalent to Lasso regression. Any value in between (0 and 1) allows for a mixture of both. Œª Œª controls the overall strength of the regularization. In R, you can fit an elastic net model using the glmnet package. Here‚Äôs a brief example: # Install and load the glmnet package if not already installed # install.packages(&quot;glmnet&quot;) library(glmnet) # Generate some example data set.seed(42) n &lt;- 100 p &lt;- 10 X &lt;- matrix(rnorm(n * p), nrow = n, ncol = p) beta_true &lt;- c(2, 0, 1, 0, 0, 3, 0, 0, -2, 0) y &lt;- X %*% beta_true + rnorm(n) # Fit an elastic net model alpha &lt;- 0.5 # You can adjust alpha to control the mixture of L1 and L2 regularization lambda &lt;- 0.1 # You can adjust lambda to control the overall strength of regularization enet_model &lt;- cv.glmnet(X, y, alpha = alpha, lambda = lambda) 1.2 Survival Analysis 1.2.1 References Web Sources Emily C. Zabor: Survival Aanalysis in R A short course on Survival Analysis applied to the Financial Industry Survival Analysis in R For Beginners Survival Analysis with R Survival Analysis Basics Steps to perform Survival Analysis in R Survival Analysis with R, Harvard Survival Analysis: Lisa Sullivan, PhD Survival Analysis Part I: Basic concepts and first analyses Practical Guide Paper An Introduction to Survival Statistics: Kaplan-Meier Analysis Kaplan Meier curves: an introduction NCCTG Lung Cancer Data pdf references 1. Data Setup Alternative 2. Introduction to Survival Analysis in Practice 3. Chapter 7 - Survival Models 4. Notes from Pat 5. Parametric Survival Models 6. Retain Customers by Churn model 7. Intro to Survival Analysis 1.2.2 Time to event analysis Time to event analysis has also been used widely in the social sciences where interest is on analyzing time to events such as job changes, marriage, birth of children and so forth. There are certain aspects of survival analysis data, such as censoring and non-normality, that generate great difficulty when trying to analyze the data using traditional statistical models such as multiple linear regression. The non-normality aspect of the data violates the normality assumption of most commonly used statistical model such as regression or ANOVA, etc. A censored observation is defined as an observation with incomplete information. When an observation is right censored it means that the information is incomplete because the subject did not have an event during the time that the subject was part of the study. The point of survival analysis is to follow subjects over time and observe at which point in time they experience the event of interest. It often happens that the study does not span enough time in order to observe the event for all the subjects in the study. This could be due to a number of reasons. Perhaps subjects drop out of the study for reasons unrelated to the study (i.e.¬†patients moving to another area and leaving no forwarding address). The common feature of all of these examples is that if the subject had been able to stay in the study then it would have been possible to observe the time of the event eventually. Type of censoring - Right truncation - Right censoring - Left truncation - Left censoring In survival analysis, censoring refers to situations where the event of interest (e.g., death, failure, or another outcome) is not observed for some subjects during the study period. There are two main types of censoring: right truncation and right censoring. Right Truncation: Definition: Right truncation occurs when individuals enter the study at different times, and some individuals have already experienced the event of interest before the study begins. Example: Consider a study on the time until a machine fails. If the study starts at a certain date, and some machines have already failed before that date, those machines are considered right-truncated because their failure times are not observed in the study. Right Censoring: Definition: Right censoring occurs when individuals are followed for a certain period, but the event of interest does not occur for some of them by the end of the study. Example: In a clinical trial studying the time until disease recurrence, if a patient has not experienced recurrence by the end of the study period or is lost to follow-up, their survival time is right-censored. The exact time of recurrence is not known for these patients. In summary, right truncation involves incomplete observation due to some subjects entering the study late, whereas right censoring occurs when the event of interest has not occurred for some subjects by the end of the study. Both types of censoring are common in survival analysis and need to be appropriately accounted for in statistical models to obtain unbiased estimates of survival probabilities and hazard rates. What is survival data? Time-to-event data that consist of a distinct start time and end time. Examples from cancer ‚Ä¢ Time from surgery to death ‚Ä¢ Time from start of treatment to progression ‚Ä¢ Time from response to recurrence Examples from other fields Time-to-event data are common in many fields including, but not limited to ‚Ä¢ Time from HIV infection to development of AIDS ‚Ä¢ Time to heart attack ‚Ä¢ Time to onset of substance abuse ‚Ä¢ Time to initiation of sexual activity ‚Ä¢ Time to machine malfunction Types of censoring A subject may be censored due to: ‚Ä¢ Loss to follow-up ‚Ä¢ Withdrawal from study ‚Ä¢ No event by end of fixed study period Specifically these are examples of right censoring. Left censoring and interval censoring are also possible, and methods exist to analyze this type of data. 1.2.2.1 Survival rate Suppose you‚Äôre a dental technician and you want to study the ‚Äúsurvival time‚Äù of a filling in a tooth. So your start time is the moment when a person goes to the dentist for a filling, and your end time, the event, is the moment when the filling breaks. The time between these two events is the focus of your study. For example, you may be interested in the probability that your filling will last longer than 5 years. To do this, you read off the value at 5 years on the graph, which is the survival rate. At 5 years, the Kaplan-Meier curve gives you a value of 0.7. So there is a 70% chance that your filling will last longer than 5 years. library(knitr) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(survival) library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.1 library(tibble) # devtools::install_github(&quot;zabore/ezfun&quot;) ezfun::set_ccf_palette(&quot;contrast&quot;) ## &lt;environment: R_GlobalEnv&gt; 1.2.2.2 The basics Original article Survival data are time-to-event data that consist of a distinct start time and end time. Examples from cancer: Time from surgery to death Time from start of treatment to progression Time from response to recurrence Time-to-event data are common in many other fields. Some other examples include: Time from HIV infection to development of AIDS Time to heart attack Time to onset of substance abuse Time to initiation of sexual activity Time to machine malfunction Because time-to-event data are common in many fields, it also goes by names besides survival analysis including: Reliability analysis Duration analysis Event history analysis Time-to-event analysis A key feature of survival data is censoring. Censoring occurs if a subject has not experienced the event of interest by the end of data collection. A subject may be censored due to: Loss to follow-up Withdrawal from study No event by end of fixed study period Specifically these are examples of right censoring. Left censoring and interval censoring are also possible, and methods exist to analyze these types of data, but this tutorial will be focus on right censoring. To illustrate the impact of censoring, suppose we have the following data: How would we compute the proportion who are event-free at 10 years? Subjects 6 and 7 were event-free at 10 years. Subjects 2, 9, and 10 had the event before 10 years. Subjects 1, 3, 4, 5, and 8 were censored before 10 years, so we don‚Äôt know whether they had the event or not at 10 years. But we know something about them - that they were each followed for a certain amount of time without the event of interest prior to being censored. Survival analysis techniques provide a way to appropriately account for censored patients in the analysis. # install.packages(c(&quot;lubridate&quot;, &quot;ggsurvfit&quot;, &quot;gtsummary&quot;, &quot;tidycmprsk&quot;)) library(lubridate) ## Warning: package &#39;lubridate&#39; was built under R version 4.3.1 ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(ggsurvfit) ## Warning: package &#39;ggsurvfit&#39; was built under R version 4.3.1 library(gtsummary) ## #StandWithUkraine library(tidycmprsk) ## Warning: package &#39;tidycmprsk&#39; was built under R version 4.3.1 ## ## Attaching package: &#39;tidycmprsk&#39; ## The following object is masked from &#39;package:gtsummary&#39;: ## ## trial # devtools::install_github(&quot;zabore/condsurv&quot;) library(condsurv) The lung dataset Throughout this section, we will use the lung dataset from the survival package as example data. The data contain subjects with advanced lung cancer from the North Central Cancer Treatment Group. We will focus on the following variables throughout this tutorial: **time:** Observed survival time in days **status:** censoring status 1=censored, 2=dead **sex:** 1=Male, 2=Female head(lung[, c(&quot;time&quot;, &quot;status&quot;, &quot;sex&quot;)]) ## time status sex ## 1 306 2 1 ## 2 455 2 1 ## 3 1010 1 1 ## 4 210 2 1 ## 5 883 2 1 ## 6 1022 1 1 Note that the status is coded in a non-standard way in this dataset. Typically you will see 1=event, 0=censored. Let‚Äôs recode it to avoid confusion: lung1 &lt;- lung %&gt;% mutate( status = recode(status, &#39;1&#39; = 0, &#39;2&#39; = 1) ) head(lung[, c(&quot;time&quot;, &quot;status&quot;, &quot;sex&quot;)]) ## time status sex ## 1 306 2 1 ## 2 455 2 1 ## 3 1010 1 1 ## 4 210 2 1 ## 5 883 2 1 ## 6 1022 1 1 Now we have: time: Observed survival time in days status: censoring status 0=censored, 1=dead sex: 1=Male, 2=Female Note: the Surv() function in the {survival} package accepts by default TRUE/FALSE, where TRUE is event and FALSE is censored; 1/0 where 1 is event and 0 is censored; or 2/1 where 2 is event and 1 is censored. Please take care to ensure the event indicator is properly formatted. Calculating survival times Data will often come with start and end dates rather than pre-calculated survival times. The first step is to make sure these are formatted as dates in R. Let‚Äôs create a small example dataset with variables sx_date for surgery date and last_fup_date for the last follow-up date: date_ex &lt;- tibble( sx_date = c(&quot;2007-06-22&quot;, &quot;2004-02-13&quot;, &quot;2010-10-27&quot;), last_fup_date = c(&quot;2017-04-15&quot;, &quot;2018-07-04&quot;, &quot;2016-10-31&quot;) ) date_ex ## # A tibble: 3 √ó 2 ## sx_date last_fup_date ## &lt;chr&gt; &lt;chr&gt; ## 1 2007-06-22 2017-04-15 ## 2 2004-02-13 2018-07-04 ## 3 2010-10-27 2016-10-31 We see these are both character variables, but we need them to be formatted as dates. We will use the {lubridate} package to work with dates. In this case, we need to use the ymd() function to change the format, since the dates are currently in the character format where the year comes first, followed by the month, and followed by the day. date_ex1 &lt;- date_ex %&gt;% mutate( sx_date = ymd(sx_date), last_fup_date = ymd(last_fup_date) ) date_ex1 ## # A tibble: 3 √ó 2 ## sx_date last_fup_date ## &lt;date&gt; &lt;date&gt; ## 1 2007-06-22 2017-04-15 ## 2 2004-02-13 2018-07-04 ## 3 2010-10-27 2016-10-31 Now that the dates are formatted, we need to calculate the difference between start and end dates in some units, usually months or years. Using the {lubridate} package, the operator %--% designates a time interval, which is then converted to the number of elapsed seconds using as.duration() and finally converted to years by dividing by dyears(1), which gives the number of seconds in a year. date_ex2 &lt;- date_ex1 %&gt;% mutate( observed_yrs = as.duration(sx_date %--% last_fup_date) / dyears(1) ) date_ex2 ## # A tibble: 3 √ó 3 ## sx_date last_fup_date observed_yrs ## &lt;date&gt; &lt;date&gt; &lt;dbl&gt; ## 1 2007-06-22 2017-04-15 9.82 ## 2 2004-02-13 2018-07-04 14.4 ## 3 2010-10-27 2016-10-31 6.01 1.2.2.3 Creating survival objects and curves The Kaplan-Meier method is the most common way to estimate survival times and probabilities. It is a non-parametric approach that results in a step function, where there is a step down each time an event occurs. Lets see the data again: lung[, c(&quot;time&quot;, &quot;status&quot;)][1:5, ] ## time status ## 1 306 2 ## 2 455 2 ## 3 1010 1 ## 4 210 2 ## 5 883 2 The Surv() function from the {survival} package creates a survival object for use as the response in a model formula. There will be one entry for each subject that is the survival time, which is followed by a + if the subject was censored. Let‚Äôs look at the first 10 observations: Surv(lung$time, lung$status)[1:10] ## [1] 306 455 1010+ 210 883 1022+ 310 361 218 166 We see that subject 1 had an event at time 306 days, subject 2 had an event at time 455 days, subject 3 was censored at time 1010 days, etc. The survfit() function creates survival curves using the Kaplan-Meier method based on a formula. Let‚Äôs generate the overall survival curve for the entire cohort, assign it to object s1, and look at the structure using str(): s1 &lt;- survfit(Surv(time, status) ~ 1, data = lung) str(s1) ## List of 16 ## $ n : int 228 ## $ time : num [1:186] 5 11 12 13 15 26 30 31 53 54 ... ## $ n.risk : num [1:186] 228 227 224 223 221 220 219 218 217 215 ... ## $ n.event : num [1:186] 1 3 1 2 1 1 1 1 2 1 ... ## $ n.censor : num [1:186] 0 0 0 0 0 0 0 0 0 0 ... ## $ surv : num [1:186] 0.996 0.982 0.978 0.969 0.965 ... ## $ std.err : num [1:186] 0.0044 0.00885 0.00992 0.01179 0.01263 ... ## $ cumhaz : num [1:186] 0.00439 0.0176 0.02207 0.03103 0.03556 ... ## $ std.chaz : num [1:186] 0.00439 0.0088 0.00987 0.01173 0.01257 ... ## $ type : chr &quot;right&quot; ## $ logse : logi TRUE ## $ conf.int : num 0.95 ## $ conf.type: chr &quot;log&quot; ## $ lower : num [1:186] 0.987 0.966 0.959 0.947 0.941 ... ## $ upper : num [1:186] 1 1 0.997 0.992 0.989 ... ## $ call : language survfit(formula = Surv(time, status) ~ 1, data = lung) ## - attr(*, &quot;class&quot;)= chr &quot;survfit&quot; 1.2.2.4 Kaplan-Meier plots/Curves The Kaplan Meier curve graphically represent the survival rate or survival function. Time is plotted on the x-axis and the survival rate is plotted on the y-axis. We will use the {ggsurvfit} package to generate Kaplan-Meier plots. This package aims to ease plotting of time-to-event endpoints using the power of the {ggplot2} package. See http://www.danieldsjoberg.com/ggsurvfit/index.html for details. Note: alternatively, survival plots can be created using base R or the {survminer} package. The {ggsurvfit} package works best if you create the survfit object using the included ggsurvfit::survfit2() function, which uses the same syntax to what we saw previously with survival::survfit(). The ggsurvfit::survfit2() tracks the environment from the function call, which allows the plot to have better default values for labeling and p-value reporting. survfit2(Surv(time, status) ~ 1, data = lung) %&gt;% ggsurvfit() + labs( x = &quot;Days&quot;, y = &quot;Overall survival probability&quot; ) The default plot in ggsurvfit() shows the step function only. We can add the confidence interval using add_confidence_interval(): survfit2(Surv(time, status) ~ 1, data = lung) %&gt;% ggsurvfit() + labs( x = &quot;Days&quot;, y = &quot;Overall survival probability&quot; ) + add_confidence_interval() Typically we will also want to see the numbers at risk in a table below the x-axis. We can add this using add_risktable(): survfit2(Surv(time, status) ~ 1, data = lung) %&gt;% ggsurvfit() + labs( x = &quot;Days&quot;, y = &quot;Overall survival probability&quot; ) + add_confidence_interval() + add_risktable() 1.2.3 Prosper Loan data library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last # web &lt;- &quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/prosperLoanData.csv&quot; # loan &lt;- fread(web) # head(loan)[, c(51, 65, 6, 7, 19, 18, 50)] 1.2.4 Customer Churn It usually costs more to acquire a customer than it does to retain a customer. Focusing on customer retention enables companies to maximize customer revenue over their lifetime. These models are seldom done optimally as they rely on binary classification flags (churn yes or no). Churn classification models do not tell WHEN a customer is likely to leave but only indicate that it‚Äôs going to happen within a certain number of days or months. In the churn classification model, we dont usually account for the differences in time. It is probably a mistake to treat a customer that is at risk of leaving in 40 days the same as a customer that remains for over a 100 days. Traditional churn modeling does not make this differentiation. As it fails to account for time, we have no clear idea at what point a marketing intervention is needed and it causes preventable customer attrition. The only point in time here is the ‚Äúwithin 40 days‚Äù threshold. As it fails to account for time, we have no clear idea at what point a marketing intervention is needed and it causes preventable customer attrition. 1.2.4.1 Re-framing the Problem to Know When Rather then use a binary classifier, we are going to re-frame the problem as time-dependent one. This enables us to intervene at the right time to stop customer attrition before it happens. No longer relying on thresholds, we now set churn as continuous time conditioned event. As the below graph shows, we now know the time that attrition risk is most likely to happen. No longer is time held constant, we now track risk over time to determine when a marketing intervention is needed to retain the customer. If we model for both the time and event, the right moment to intervene and prevent attrition is apparent. A modeling technique called Survival Analysis allows for us to do this and with the advent of modern Machine Learning, it‚Äôs now a trivial task. %reload_ext autoreload %autoreload 2 %matplotlib inline import xgboost as xgb import shap import sksurv.metrics as surv_metrics from sksurv.datasets import get_x_y from lifelines import KaplanMeierFitter from lifelines.plotting import plot_lifetimes import numpy as np import pandas as pd import seaborn as sns from matplotlib import pyplot as plt from sklearn.compose import ColumnTransformer from sklearn.exceptions import DataConversionWarning from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler plt.rcParams[&#39;figure.figsize&#39;] = [7.2, 4.8] pd.set_option(&quot;display.float_format&quot;, lambda x: &quot;%.4f&quot; % x) sns.set_style(&#39;darkgrid&#39;) SEED = 123 df = pd.read_csv(&quot;../../data/churn.txt&quot;) # denoting churn and duration df[&quot;event&quot;] = np.where(df[&quot;churn?&quot;] == &quot;False.&quot;, 0, 1) df = df.rename(columns={&quot;account_length&quot;: &quot;duration&quot;}) del df[&#39;churn?&#39;] df = df.dropna() df = df.drop_duplicates() df.head() print(&quot;Total Records:&quot;,df.shape[0],&quot;\\n&quot;) print(&quot;Percent Churn Rate:&quot;,df.event.mean()) print(&quot;&quot;) print(&quot;Duration Intervals&quot;) print(df[&#39;duration&#39;].describe()) For Survival models data is different from a traditional classification problem and requires: - A Censor ‚Äî For our purposes these are customers who‚Äôve yet to churn. Read about right censoring here. Duration ‚Äî The duration or time t of the customer‚Äôs activity. In this case, it‚Äôs Account Length in days. Event ‚Äî The binary target, in this case if they terminated their phone plan marked by Churn? . ax = plot_lifetimes(df.head(10)[&#39;duration&#39;], df.head(10)[&#39;event&#39;]) _=ax.set_xlabel(&quot;Duration: Account Length (days)&quot;) _=ax.set_ylabel(&quot;Customer Number&quot;) _=ax.set_title(&quot;Observed Customer Attrition&quot;) In the above plot, the red lines indicates when a customer has left with the dots indicating the specific point in time. Blue lines are customers that are still active up to the time measured on the x-axis in Duration. Here we see that customer number 8 did not attrit until up to 195 days, with customer numbers 0 and 4 leaving in 163 and 146 days respectively. All other customers are still active. Notice how all customers are set on the same time scale because the data is analytically aligned. Each customer might have come in at different times but we‚Äôve set the days as the same. This is what allowed us to right-censor the data on the churn event. Real world data needs both censoring and aligning before modeling can begin. 1.2.4.2 The Risk of Churn A more informative approach might be to estimate the Survival Function or the time in days a customer has until they attrit. For this purpose, we will use a Kaplan Meier Estimator to calculate how long until attrition occurs. The estimator is defined as: Where \\(ùëë_ùëñ\\) are the number of churn events at time \\(ùë°\\) and \\(ùëõ_ùëñ\\) is the number of customers at risk of churn just prior to time \\(ùë°\\). We will use the great python package lifelines to plot the Survival Function as the function is a component of the final churn model. kmf = KaplanMeierFitter() kmf.fit(df[&#39;duration&#39;], event_observed=df[&#39;event&#39;]) kmf.plot_survival_function() _=plt.title(&#39;Survival Function for Telco Churn&#39;); _=plt.xlabel(&quot;Duration: Account Length (days)&quot;) _=plt.ylabel(&quot;Churn Risk (Percent Churned)&quot;) _=plt.axvline(x=kmf.median_survival_time_, color=&#39;r&#39;,linestyle=&#39;--&#39;) Let‚Äôs look at the median survival time. This is the point by which half of customers have churned out. According to this graph, where it‚Äôs marked by the red dotted line, by about 152 days half of customers churn. This is helpful because it gives overall baseline when intervention is needed. However, for each individual customer this is uninformative. What is missing is the point in time in which churn risk is highest for each customer. For that we will create a model using Cox‚Äôs Proportional Hazard which uses a log-risk function \\(h(x)\\). The Hazard function is conditioned on rate of a customers remaining until time t or later, this allows to estimate the risk of churn overtime. This will enable us to score each customer and anticipate when a marketing intervention is needed. However, before we proceed to that, we need to preprocess the data. 1.2.4.3 Data Splitting and Preprocessing First we will split the data into training and testing. We‚Äôll use the testing set as the validation for the example. In practice, you want all three of these splits so that you don‚Äôt tune to the validation set. Next, we take the numeric features and categorical features and then preprocess them for downstream modeling. In the case of categories, we will first impute with the constant and then simply one-hot encode them. In the case of numerics, we will fill with the median then standardize them between values of 0 and 1. This is all wrapped into Sklearn‚Äôs Pipeline and ColumnTransformer for simplicity‚Äôs sake. As part of the Churn Pipeline all these steps are included with the final preprocessor saved for use at inference time. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
